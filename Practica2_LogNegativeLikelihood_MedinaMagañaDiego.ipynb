{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP96BTGlQ0eRs6A8p0EFwlq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AquilesClavel/RedesNeuronales/blob/main/Practica2_LogNegativeLikelihood_MedinaMaga%C3%B1aDiego.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvRA9_K-MPA6",
        "outputId": "be963b30-1725-4e49-8605-93e4e8f36a8e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La mejor línea es de -2.5935920078268282e-11*x + -4.997962871344797e-07\n",
            "LNL: 5.545177444479812\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#Constante para la aproximación de la derivada\n",
        "h=0.000001\n",
        "valsX = np.array([\n",
        "    [0, 0, 1, 1],  # valsX[0]\n",
        "    [0, 1, 0, 1]   # valsX[1]\n",
        "])\n",
        "valsY = np.array([1,0,0,1])\n",
        "\n",
        "\n",
        "\n",
        "def g(m,x,b):\n",
        "  x= np.array(x)\n",
        "  linea = m*x+b\n",
        "  return 1/(1+np.exp(-linea))\n",
        "\n",
        "def LNL(m,x,b,y):#MSE\n",
        "    res = -(np.sum((y*np.log(g(m,x,b)))+((1-y)*np.log(1-g(m,x,b)))))\n",
        "    return res\n",
        "\n",
        "\n",
        "def aproximaciónT0(m,b):\n",
        "    return (LNL(m+h,valsX,b,valsY)-LNL(m,valsX,b,valsY))/h\n",
        "\n",
        "def aproximaciónT1(m,b):\n",
        "    return (LNL(m,valsX,b+h,valsY)-LNL(m,valsX,b,valsY))/h\n",
        "\n",
        "\n",
        "#gradiente descendiente\n",
        "\n",
        "def grad_descend(mse,startT0,startT1,learning_rate,num_iter):\n",
        "    t0 = startT0\n",
        "    t1 = startT1\n",
        "\n",
        "    for i in range(num_iter):\n",
        "        t0 = t0-learning_rate*aproximaciónT0(t0,t1)\n",
        "        t1 = t1-learning_rate*aproximaciónT1(t0,t1)\n",
        "\n",
        "    return t0,t1\n",
        "\n",
        "startT0 = np.random.rand()\n",
        "startT1 = np.random.rand()\n",
        "learning_rate = 0.09\n",
        "num_iter = 1000\n",
        "\n",
        "m,b= grad_descend(LNL,startT0,startT1,learning_rate,num_iter)\n",
        "\n",
        "# Mostrar resultados\n",
        "print(f\"La mejor línea es de {m}*x + {b}\")\n",
        "print(f\"LNL: {LNL(m, valsX, b, valsY)}\")\n",
        "\n"
      ]
    }
  ]
}